{"id":"camera.js","dependencies":[{"name":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/package.json","includedInParent":true,"mtime":1726467485028},{"name":"@tensorflow-models/posenet","loc":{"line":1,"column":32},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/node_modules/@tensorflow-models/posenet/dist/posenet.esm.js"},{"name":"@tensorflow-models/facemesh","loc":{"line":2,"column":33},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/node_modules/@tensorflow-models/facemesh/dist/facemesh.esm.js"},{"name":"@tensorflow/tfjs","loc":{"line":3,"column":20},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/node_modules/@tensorflow/tfjs/dist/tf.esm.js"},{"name":"paper","loc":{"line":4,"column":23},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/node_modules/paper/dist/paper-full.js"},{"name":"stats.js","loc":{"line":5,"column":18},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/node_modules/stats.js/build/stats.min.js"},{"name":"babel-polyfill","loc":{"line":6,"column":7},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/node_modules/babel-polyfill/lib/index.js"},{"name":"./utils/demoUtils","loc":{"line":8,"column":95},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/utils/demoUtils.js"},{"name":"./utils/svgUtils","loc":{"line":9,"column":23},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/utils/svgUtils.js"},{"name":"./illustrationGen/illustration","loc":{"line":10,"column":31},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/illustrationGen/illustration.js"},{"name":"./illustrationGen/skeleton","loc":{"line":11,"column":43},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/illustrationGen/skeleton.js"},{"name":"./utils/fileUtils","loc":{"line":12,"column":24},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/utils/fileUtils.js"},{"name":"./resources/illustration/girl.svg","loc":{"line":14,"column":25},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/resources/illustration/girl.svg"},{"name":"./resources/illustration/boy.svg","loc":{"line":15,"column":24},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/resources/illustration/boy.svg"},{"name":"./resources/illustration/abstract.svg","loc":{"line":16,"column":29},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/resources/illustration/abstract.svg"},{"name":"./resources/illustration/blathers.svg","loc":{"line":17,"column":29},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/resources/illustration/blathers.svg"},{"name":"./resources/illustration/tom-nook.svg","loc":{"line":18,"column":28},"parent":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/camera.js","resolved":"/home/eric/Documents/code/blockchain/streamr/streamr-experientialnft/poc/subscribe/streamer/resources/illustration/tom-nook.svg"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.bindPage = bindPage;\n\nvar posenet_module = _interopRequireWildcard(require(\"@tensorflow-models/posenet\"));\n\nvar facemesh_module = _interopRequireWildcard(require(\"@tensorflow-models/facemesh\"));\n\nvar tf = _interopRequireWildcard(require(\"@tensorflow/tfjs\"));\n\nvar paper = _interopRequireWildcard(require(\"paper\"));\n\nvar _stats = _interopRequireDefault(require(\"stats.js\"));\n\nrequire(\"babel-polyfill\");\n\nvar _demoUtils = require(\"./utils/demoUtils\");\n\nvar _svgUtils = require(\"./utils/svgUtils\");\n\nvar _illustration2 = require(\"./illustrationGen/illustration\");\n\nvar _skeleton = require(\"./illustrationGen/skeleton\");\n\nvar _fileUtils = require(\"./utils/fileUtils\");\n\nvar girlSVG = _interopRequireWildcard(require(\"./resources/illustration/girl.svg\"));\n\nvar boySVG = _interopRequireWildcard(require(\"./resources/illustration/boy.svg\"));\n\nvar abstractSVG = _interopRequireWildcard(require(\"./resources/illustration/abstract.svg\"));\n\nvar blathersSVG = _interopRequireWildcard(require(\"./resources/illustration/blathers.svg\"));\n\nvar tomNookSVG = _interopRequireWildcard(require(\"./resources/illustration/tom-nook.svg\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache() { if (typeof WeakMap !== \"function\") return null; var cache = new WeakMap(); _getRequireWildcardCache = function () { return cache; }; return cache; }\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction _createForOfIteratorHelper(o) { if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (o = _unsupportedIterableToArray(o))) { var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var it, normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(n); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\nfunction asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { Promise.resolve(value).then(_next, _throw); } }\n\nfunction _asyncToGenerator(fn) { return function () { var self = this, args = arguments; return new Promise(function (resolve, reject) { var gen = fn.apply(self, args); function _next(value) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"next\", value); } function _throw(err) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"throw\", err); } _next(undefined); }); }; }\n\n// Camera stream video element\nvar video;\nvar videoWidth = 300;\nvar videoHeight = 300; // Canvas\n\nvar faceDetection = null;\nvar illustration = null;\nvar peerIllustration = null;\nvar canvasScope;\nvar canvasWidth = 750;\nvar canvasHeight = 550;\nvar illustrations = []; // ML models\n\nvar facemesh;\nvar posenet;\nvar minPartConfidence = 0.1;\nvar nmsRadius = 30.0;\nvar peerPose = null; // Misc\n\nvar mobile = false;\nvar stats = new _stats.default();\nvar avatarSvgs = {\n  'girl': girlSVG.default,\n  'boy': boySVG.default,\n  'abstract': abstractSVG.default,\n  'blathers': blathersSVG.default,\n  'tom-nook': tomNookSVG.default\n};\nvar apiKey = 'YmJhOTAxNGRmYjJkNDk1YWEzMmQxZTBlZmM4YTM5M2I';\nvar streamId = encodeURIComponent('0x5dbef432d012c8d20993214f2c3765e9cf83d180/signature-amoy');\nvar sub = new WebSocket(\"wss://adjusted-bass-scarcely.ngrok-free.app/streams/\".concat(streamId, \"/subscribe?apiKey=\").concat(apiKey));\nvar videoElement = document.getElementById('video');\nvar buffer = []; // Buffer to store incoming base64 video segments\n\nsub.onopen = function () {\n  console.log(\"WebSocket connection established\");\n}; // Handle incoming WebSocket messages with base64 video data\n\n\nsub.onmessage = function (event) {\n  console.log('Received message:', event.data);\n\n  try {\n    var parsedData = JSON.parse(event.data);\n\n    if (parsedData.video) {\n      var base64Data = parsedData.video.trim();\n      console.log('Base64 string length:', base64Data.length); // Add the base64 video segment to the buffer\n\n      buffer.push(base64Data);\n    } else {\n      console.error('No video data in the WebSocket message');\n    }\n  } catch (error) {\n    console.error('Error processing video segment:', error);\n  }\n}; // Handle WebSocket connection errors\n\n\nsub.onerror = function (error) {\n  console.error('WebSocket Error:', error);\n}; // Handle WebSocket connection closure\n\n\nsub.onclose = function () {\n  console.log('WebSocket connection closed');\n};\n\nfunction setupVideo() {\n  return _setupVideo.apply(this, arguments);\n}\n\nfunction _setupVideo() {\n  _setupVideo = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee2() {\n    var video, base64Data, byteCharacters, byteNumbers, i, byteArray, blob, blobURL;\n    return regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            video = document.getElementById('video');\n            video.width = videoWidth;\n            video.height = videoHeight; // Wait until the buffer has data\n\n          case 3:\n            if (!(buffer.length === 0)) {\n              _context2.next = 9;\n              break;\n            }\n\n            console.log('Buffer is empty, waiting for new segments...');\n            _context2.next = 7;\n            return new Promise(function (resolve) {\n              return setTimeout(resolve, 50);\n            });\n\n          case 7:\n            _context2.next = 3;\n            break;\n\n          case 9:\n            base64Data = buffer.shift(); // Remove the oldest segment from the buffer\n            // Create a Blob from the base64 video text\n\n            byteCharacters = atob(base64Data);\n            byteNumbers = new Array(byteCharacters.length);\n\n            for (i = 0; i < byteCharacters.length; i++) {\n              byteNumbers[i] = byteCharacters.charCodeAt(i);\n            }\n\n            byteArray = new Uint8Array(byteNumbers);\n            blob = new Blob([byteArray], {\n              type: 'video/mp4'\n            }); // Create a URL for the blob and set it as the video source\n\n            blobURL = URL.createObjectURL(blob);\n            video.src = blobURL;\n            return _context2.abrupt(\"return\", new Promise(function (resolve) {\n              video.onloadedmetadata = function () {\n                resolve(video);\n              };\n            }));\n\n          case 18:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n  return _setupVideo.apply(this, arguments);\n}\n\nfunction loadVideo() {\n  return _loadVideo.apply(this, arguments);\n}\n\nfunction _loadVideo() {\n  _loadVideo = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee3() {\n    var video;\n    return regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            _context3.next = 2;\n            return setupVideo();\n\n          case 2:\n            video = _context3.sent;\n            video.play();\n            return _context3.abrupt(\"return\", video);\n\n          case 5:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n  return _loadVideo.apply(this, arguments);\n}\n\nvar defaultPoseNetArchitecture = 'MobileNetV1';\nvar defaultQuantBytes = 2;\nvar defaultMultiplier = 1.0;\nvar defaultStride = 16;\nvar defaultInputResolution = 200;\nvar guiState = {\n  avatarSVG: Object.keys(avatarSvgs)[0],\n  debug: {\n    showDetectionDebug: true,\n    showIllustrationDebug: false\n  }\n};\n/**\n * Sets up dat.gui controller on the top-right of the window\n */\n\nfunction setupGui(cameras) {\n  if (cameras.length > 0) {\n    guiState.camera = cameras[0].deviceId;\n  }\n}\n/**\n * Feeds an image to posenet to estimate poses - this is where the magic\n * happens. This function loops with a requestAnimationFrame method.\n */\n\n\nfunction detectPoseInRealTime(video) {\n  var canvas = document.getElementById('output');\n  var keypointCanvas = document.getElementById('keypoints');\n  var videoCtx = canvas.getContext('2d');\n  var keypointCtx = keypointCanvas.getContext('2d');\n  canvas.width = videoWidth;\n  canvas.height = videoHeight;\n  keypointCanvas.width = videoWidth;\n  keypointCanvas.height = videoHeight;\n\n  function poseDetectionFrame() {\n    return _poseDetectionFrame.apply(this, arguments);\n  }\n\n  function _poseDetectionFrame() {\n    _poseDetectionFrame = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee() {\n      var poses, input, all_poses;\n      return regeneratorRuntime.wrap(function _callee$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              // Begin monitoring code for frames per second\n              stats.begin();\n              poses = [];\n              videoCtx.clearRect(0, 0, videoWidth, videoHeight); // Draw video\n\n              videoCtx.save();\n              videoCtx.scale(-1, 1);\n              videoCtx.translate(-videoWidth, 0);\n              videoCtx.drawImage(video, 0, 0, videoWidth, videoHeight);\n              videoCtx.restore(); // Creates a tensor from an image\n\n              input = tf.browser.fromPixels(canvas);\n              _context.next = 11;\n              return facemesh.estimateFaces(input, false, false);\n\n            case 11:\n              faceDetection = _context.sent;\n              _context.next = 14;\n              return posenet.estimatePoses(video, {\n                flipHorizontal: true,\n                decodingMethod: 'multi-person',\n                maxDetections: 1,\n                scoreThreshold: minPartConfidence,\n                nmsRadius: nmsRadius\n              });\n\n            case 14:\n              all_poses = _context.sent;\n              poses = poses.concat(all_poses);\n              input.dispose();\n              keypointCtx.clearRect(0, 0, videoWidth, videoHeight);\n              canvasScope.project.clear();\n\n              if (poses.length >= 1 && illustrations) {\n                _skeleton.Skeleton.flipPose(poses[0]);\n\n                illustrations[1].updateSkeleton(poses[0], null);\n                illustrations[1].draw(canvasScope, videoWidth, videoHeight);\n              }\n\n              if (peerPose) {\n                _skeleton.Skeleton.flipPose(peerPose[0]);\n\n                illustrations[0].updateSkeleton(peerPose[0], null);\n                illustrations[0].draw(canvasScope, videoWidth, videoHeight);\n              }\n\n              canvasScope.project.activeLayer.scale(canvasWidth / videoWidth, canvasHeight / videoHeight, new canvasScope.Point(0, 0)); // End monitoring code for frames per second\n\n              stats.end();\n              requestAnimationFrame(poseDetectionFrame);\n\n            case 24:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, _callee);\n    }));\n    return _poseDetectionFrame.apply(this, arguments);\n  }\n\n  poseDetectionFrame();\n}\n\nfunction setupCanvas() {\n  mobile = (0, _demoUtils.isMobile)();\n\n  if (mobile) {\n    canvasWidth = Math.min(window.innerWidth, window.innerHeight);\n    canvasHeight = canvasWidth;\n    videoWidth *= 0.7;\n    videoHeight *= 0.7;\n  }\n\n  canvasScope = paper.default;\n  var canvas = document.querySelector('.illustration-canvas');\n  ;\n  canvas.width = canvasWidth;\n  canvas.height = canvasHeight;\n  canvasScope.setup(canvas); // Setup peer illustration canvas\n\n  peerIllustration = new _illustration2.PoseIllustration(canvasScope, {\n    x: 150,\n    y: 150\n  });\n}\n/**\n * Kicks off the demo by loading the posenet model, finding and loading\n * available camera devices, and setting off the detectPoseInRealTime function.\n */\n\n\nfunction bindPage() {\n  return _bindPage.apply(this, arguments);\n}\n\nfunction _bindPage() {\n  _bindPage = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee4() {\n    var t0, info;\n    return regeneratorRuntime.wrap(function _callee4$(_context4) {\n      while (1) {\n        switch (_context4.prev = _context4.next) {\n          case 0:\n            setupCanvas();\n            (0, _demoUtils.toggleLoadingUI)(true);\n            (0, _demoUtils.setStatusText)('Loading PoseNet model...');\n            _context4.next = 5;\n            return posenet_module.load({\n              architecture: defaultPoseNetArchitecture,\n              outputStride: defaultStride,\n              inputResolution: defaultInputResolution,\n              multiplier: defaultMultiplier,\n              quantBytes: defaultQuantBytes\n            });\n\n          case 5:\n            posenet = _context4.sent;\n            (0, _demoUtils.setStatusText)('Loading FaceMesh model...');\n            _context4.next = 9;\n            return facemesh_module.load();\n\n          case 9:\n            facemesh = _context4.sent;\n            (0, _demoUtils.setStatusText)('Loading Avatar file...');\n            t0 = new Date();\n            _context4.next = 14;\n            return parseSVG(Object.values(avatarSvgs).slice(0, 3));\n\n          case 14:\n            (0, _demoUtils.setStatusText)('Setting up camera...');\n            _context4.prev = 15;\n            _context4.next = 18;\n            return loadVideo();\n\n          case 18:\n            video = _context4.sent;\n            _context4.next = 27;\n            break;\n\n          case 21:\n            _context4.prev = 21;\n            _context4.t0 = _context4[\"catch\"](15);\n            info = document.getElementById('info');\n            info.textContent = 'this device type is not supported yet, ' + 'or this browser does not support video capture: ' + _context4.t0.toString();\n            info.style.display = 'block';\n            throw _context4.t0;\n\n          case 27:\n            setupGui([], posenet); // setupFPS();\n\n            (0, _demoUtils.toggleLoadingUI)(false);\n            detectPoseInRealTime(video, posenet);\n\n          case 30:\n          case \"end\":\n            return _context4.stop();\n        }\n      }\n    }, _callee4, null, [[15, 21]]);\n  }));\n  return _bindPage.apply(this, arguments);\n}\n\nnavigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;\n\n_fileUtils.FileUtils.setDragDropHandler(function (result) {\n  parseSVG(result);\n});\n\nfunction parseSVG(_x) {\n  return _parseSVG.apply(this, arguments);\n}\n\nfunction _parseSVG() {\n  _parseSVG = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee5(targets) {\n    var _iterator, _step, target, svgScope, skeleton, _illustration;\n\n    return regeneratorRuntime.wrap(function _callee5$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            _iterator = _createForOfIteratorHelper(targets);\n            _context5.prev = 1;\n\n            _iterator.s();\n\n          case 3:\n            if ((_step = _iterator.n()).done) {\n              _context5.next = 14;\n              break;\n            }\n\n            target = _step.value;\n            _context5.next = 7;\n            return _svgUtils.SVGUtils.importSVG(target\n            /* SVG string or file path */\n            );\n\n          case 7:\n            svgScope = _context5.sent;\n            skeleton = new _skeleton.Skeleton(svgScope);\n            _illustration = new _illustration2.PoseIllustration(canvasScope);\n\n            _illustration.bindSkeleton(skeleton, svgScope);\n\n            illustrations.push(_illustration);\n\n          case 12:\n            _context5.next = 3;\n            break;\n\n          case 14:\n            _context5.next = 19;\n            break;\n\n          case 16:\n            _context5.prev = 16;\n            _context5.t0 = _context5[\"catch\"](1);\n\n            _iterator.e(_context5.t0);\n\n          case 19:\n            _context5.prev = 19;\n\n            _iterator.f();\n\n            return _context5.finish(19);\n\n          case 22:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee5, null, [[1, 16, 19, 22]]);\n  }));\n  return _parseSVG.apply(this, arguments);\n}\n\nbindPage();"},"sourceMaps":null,"error":null,"hash":"ffed7f0c0352ff04483b93e45fc99534","cacheData":{"env":{}}}
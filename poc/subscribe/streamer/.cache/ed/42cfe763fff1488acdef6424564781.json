{"id":"camera.js","dependencies":[{"name":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/package.json","includedInParent":true,"mtime":1726467485028},{"name":"@tensorflow-models/posenet","loc":{"line":1,"column":32},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/node_modules/@tensorflow-models/posenet/dist/posenet.esm.js"},{"name":"@tensorflow-models/facemesh","loc":{"line":2,"column":33},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/node_modules/@tensorflow-models/facemesh/dist/facemesh.esm.js"},{"name":"@tensorflow/tfjs","loc":{"line":3,"column":20},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/node_modules/@tensorflow/tfjs/dist/tf.esm.js"},{"name":"paper","loc":{"line":4,"column":23},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/node_modules/paper/dist/paper-full.js"},{"name":"stats.js","loc":{"line":5,"column":18},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/node_modules/stats.js/build/stats.min.js"},{"name":"babel-polyfill","loc":{"line":6,"column":7},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/node_modules/babel-polyfill/lib/index.js"},{"name":"./utils/demoUtils","loc":{"line":8,"column":95},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/utils/demoUtils.js"},{"name":"./utils/svgUtils","loc":{"line":9,"column":23},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/utils/svgUtils.js"},{"name":"./illustrationGen/illustration","loc":{"line":10,"column":31},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/illustrationGen/illustration.js"},{"name":"./illustrationGen/skeleton","loc":{"line":11,"column":43},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/illustrationGen/skeleton.js"},{"name":"./utils/fileUtils","loc":{"line":12,"column":24},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/utils/fileUtils.js"},{"name":"./resources/illustration/girl.svg","loc":{"line":14,"column":25},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/resources/illustration/girl.svg"},{"name":"./resources/illustration/boy.svg","loc":{"line":15,"column":24},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/resources/illustration/boy.svg"},{"name":"./resources/illustration/abstract.svg","loc":{"line":16,"column":29},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/resources/illustration/abstract.svg"},{"name":"./resources/illustration/blathers.svg","loc":{"line":17,"column":29},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/resources/illustration/blathers.svg"},{"name":"./resources/illustration/tom-nook.svg","loc":{"line":18,"column":28},"parent":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/camera.js","resolved":"/home/eric/Documents/code/blockchain/cartesi/signature/frontend/resources/illustration/tom-nook.svg"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.bindPage = bindPage;\n\nvar posenet_module = _interopRequireWildcard(require(\"@tensorflow-models/posenet\"));\n\nvar facemesh_module = _interopRequireWildcard(require(\"@tensorflow-models/facemesh\"));\n\nvar tf = _interopRequireWildcard(require(\"@tensorflow/tfjs\"));\n\nvar paper = _interopRequireWildcard(require(\"paper\"));\n\nvar _stats = _interopRequireDefault(require(\"stats.js\"));\n\nrequire(\"babel-polyfill\");\n\nvar _demoUtils = require(\"./utils/demoUtils\");\n\nvar _svgUtils = require(\"./utils/svgUtils\");\n\nvar _illustration2 = require(\"./illustrationGen/illustration\");\n\nvar _skeleton = require(\"./illustrationGen/skeleton\");\n\nvar _fileUtils = require(\"./utils/fileUtils\");\n\nvar girlSVG = _interopRequireWildcard(require(\"./resources/illustration/girl.svg\"));\n\nvar boySVG = _interopRequireWildcard(require(\"./resources/illustration/boy.svg\"));\n\nvar abstractSVG = _interopRequireWildcard(require(\"./resources/illustration/abstract.svg\"));\n\nvar blathersSVG = _interopRequireWildcard(require(\"./resources/illustration/blathers.svg\"));\n\nvar tomNookSVG = _interopRequireWildcard(require(\"./resources/illustration/tom-nook.svg\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache() { if (typeof WeakMap !== \"function\") return null; var cache = new WeakMap(); _getRequireWildcardCache = function () { return cache; }; return cache; }\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction _createForOfIteratorHelper(o) { if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (o = _unsupportedIterableToArray(o))) { var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var it, normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(n); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\nfunction asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { Promise.resolve(value).then(_next, _throw); } }\n\nfunction _asyncToGenerator(fn) { return function () { var self = this, args = arguments; return new Promise(function (resolve, reject) { var gen = fn.apply(self, args); function _next(value) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"next\", value); } function _throw(err) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"throw\", err); } _next(undefined); }); }; }\n\n// Camera stream video element\nvar video;\nvar videoWidth = 300;\nvar videoHeight = 300; // Canvas\n\nvar faceDetection = null;\nvar illustration = null;\nvar peerIllustration = null;\nvar canvasScope;\nvar canvasWidth = 750;\nvar canvasHeight = 550;\nvar illustrations = []; // ML models\n\nvar facemesh;\nvar posenet;\nvar minPartConfidence = 0.1;\nvar nmsRadius = 30.0;\nvar peerPose = null; // Misc\n\nvar mobile = false;\nvar stats = new _stats.default();\nvar avatarSvgs = {\n  'girl': girlSVG.default,\n  'boy': boySVG.default,\n  'abstract': abstractSVG.default,\n  'blathers': blathersSVG.default,\n  'tom-nook': tomNookSVG.default\n};\n/**\n * Peer connection\n */\n\nvar peer = new Peer();\nvar conn;\npeer.on('open', function (id) {\n  document.getElementById('peer-id').value = id;\n});\npeer.on('connection', function (connection) {\n  conn = connection;\n  conn.on('data', function (data) {\n    var receivedData = JSON.parse(data);\n\n    if (receivedData) {\n      updatePeerIllustration(receivedData);\n    }\n  }); // Request the current state from the peer\n\n  conn.on('open', function () {\n    conn.send(JSON.stringify({\n      type: 'request-state'\n    }));\n  });\n});\n\nfunction connect() {\n  var connectId = document.getElementById('connect-id').value;\n  conn = peer.connect(connectId);\n  conn.on('open', function () {\n    var chat = document.getElementById('chat');\n    chat.innerHTML += \"<p>Connected to peer!</p>\";\n    chat.scrollTop = chat.scrollHeight;\n    conn.on('data', function (data) {\n      var receivedData = JSON.parse(data);\n\n      if (receivedData) {\n        updatePeerIllustration(receivedData);\n      }\n    }); // Send the current state to the newly connected peer\n\n    sendState();\n  });\n}\n\nfunction sendState() {\n  if (conn && conn.open) {\n    var data = {\n      type: 'state',\n      svg: guiState.avatarSVG,\n      pose: illustration ? illustration.getSkeletonData() : null\n    };\n    conn.send(JSON.stringify(data));\n  }\n}\n\nfunction sendIllustration(data) {\n  if (conn && conn.open) {\n    var payload = {\n      type: 'pose',\n      pose: data\n    };\n    conn.send(JSON.stringify(payload));\n  }\n}\n\nwindow.connect = connect;\n\nfunction updatePeerIllustration(_x) {\n  return _updatePeerIllustration.apply(this, arguments);\n}\n/**\n * Loads a the camera to be used in the demo\n *\n */\n\n\nfunction _updatePeerIllustration() {\n  _updatePeerIllustration = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee2(data) {\n    return regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            if (data.type === 'state') {} else if (data.type === 'pose') {\n              peerPose = data.pose;\n              console.log(\"Peer pose \" + peerPose);\n            } else if (data.type === 'request-state') {\n              sendState();\n            }\n\n          case 1:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n  return _updatePeerIllustration.apply(this, arguments);\n}\n\nfunction setupCamera() {\n  return _setupCamera.apply(this, arguments);\n}\n\nfunction _setupCamera() {\n  _setupCamera = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee3() {\n    var video, stream;\n    return regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            if (!(!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia)) {\n              _context3.next = 2;\n              break;\n            }\n\n            throw new Error('Browser API navigator.mediaDevices.getUserMedia not available');\n\n          case 2:\n            video = document.getElementById('video');\n            video.width = videoWidth;\n            video.height = videoHeight;\n            _context3.next = 7;\n            return navigator.mediaDevices.getUserMedia({\n              'audio': false,\n              'video': {\n                facingMode: 'user',\n                width: videoWidth,\n                height: videoHeight\n              }\n            });\n\n          case 7:\n            stream = _context3.sent;\n            video.srcObject = stream;\n            return _context3.abrupt(\"return\", new Promise(function (resolve) {\n              video.onloadedmetadata = function () {\n                resolve(video);\n              };\n            }));\n\n          case 10:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n  return _setupCamera.apply(this, arguments);\n}\n\nfunction loadVideo() {\n  return _loadVideo.apply(this, arguments);\n}\n\nfunction _loadVideo() {\n  _loadVideo = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee4() {\n    var video;\n    return regeneratorRuntime.wrap(function _callee4$(_context4) {\n      while (1) {\n        switch (_context4.prev = _context4.next) {\n          case 0:\n            _context4.next = 2;\n            return setupCamera();\n\n          case 2:\n            video = _context4.sent;\n            video.play();\n            return _context4.abrupt(\"return\", video);\n\n          case 5:\n          case \"end\":\n            return _context4.stop();\n        }\n      }\n    }, _callee4);\n  }));\n  return _loadVideo.apply(this, arguments);\n}\n\nvar defaultPoseNetArchitecture = 'MobileNetV1';\nvar defaultQuantBytes = 2;\nvar defaultMultiplier = 1.0;\nvar defaultStride = 16;\nvar defaultInputResolution = 200;\nvar guiState = {\n  avatarSVG: Object.keys(avatarSvgs)[0],\n  debug: {\n    showDetectionDebug: true,\n    showIllustrationDebug: false\n  }\n};\n/**\n * Sets up dat.gui controller on the top-right of the window\n */\n\nfunction setupGui(cameras) {\n  if (cameras.length > 0) {\n    guiState.camera = cameras[0].deviceId;\n  } // const gui = new dat.GUI({width: 300});\n  // let multi = gui.addFolder('Image');\n  // gui.add(guiState, 'avatarSVG', Object.keys(avatarSvgs)).onChange(async () => {\n  //   await parseSVG(avatarSvgs[guiState.avatarSVG]);\n  //   // sendState();  // Notify the peer about the new SVG\n  // });\n  // multi.open();\n\n}\n/**\n * Sets up a frames per second panel on the top-left of the window\n */\n// function setupFPS() {\n//   stats.showPanel(0);  // 0: fps, 1: ms, 2: mb, 3+: custom\n//   document.getElementById('main').appendChild(stats.dom);\n// }\n\n/**\n * Feeds an image to posenet to estimate poses - this is where the magic\n * happens. This function loops with a requestAnimationFrame method.\n */\n\n\nfunction detectPoseInRealTime(video) {\n  var canvas = document.getElementById('output');\n  var keypointCanvas = document.getElementById('keypoints');\n  var videoCtx = canvas.getContext('2d');\n  var keypointCtx = keypointCanvas.getContext('2d');\n  canvas.width = videoWidth;\n  canvas.height = videoHeight;\n  keypointCanvas.width = videoWidth;\n  keypointCanvas.height = videoHeight;\n\n  function poseDetectionFrame() {\n    return _poseDetectionFrame.apply(this, arguments);\n  }\n\n  function _poseDetectionFrame() {\n    _poseDetectionFrame = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee() {\n      var poses, input, all_poses;\n      return regeneratorRuntime.wrap(function _callee$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              // Begin monitoring code for frames per second\n              stats.begin();\n              poses = [];\n              videoCtx.clearRect(0, 0, videoWidth, videoHeight); // Draw video\n\n              videoCtx.save();\n              videoCtx.scale(-1, 1);\n              videoCtx.translate(-videoWidth, 0);\n              videoCtx.drawImage(video, 0, 0, videoWidth, videoHeight);\n              videoCtx.restore(); // Creates a tensor from an image\n\n              input = tf.browser.fromPixels(canvas);\n              _context.next = 11;\n              return facemesh.estimateFaces(input, false, false);\n\n            case 11:\n              faceDetection = _context.sent;\n              _context.next = 14;\n              return posenet.estimatePoses(video, {\n                flipHorizontal: true,\n                decodingMethod: 'multi-person',\n                maxDetections: 1,\n                scoreThreshold: minPartConfidence,\n                nmsRadius: nmsRadius\n              });\n\n            case 14:\n              all_poses = _context.sent;\n              poses = poses.concat(all_poses);\n              input.dispose();\n              keypointCtx.clearRect(0, 0, videoWidth, videoHeight);\n              canvasScope.project.clear(); // Send the updated illustration data to the peer\n\n              sendIllustration(poses);\n\n              if (poses.length >= 1 && illustrations) {\n                _skeleton.Skeleton.flipPose(poses[0]);\n\n                illustrations[1].updateSkeleton(poses[0], null);\n                illustrations[1].draw(canvasScope, videoWidth, videoHeight);\n              }\n\n              if (peerPose) {\n                _skeleton.Skeleton.flipPose(peerPose[0]);\n\n                illustrations[0].updateSkeleton(peerPose[0], null);\n                illustrations[0].draw(canvasScope, videoWidth, videoHeight);\n              }\n\n              canvasScope.project.activeLayer.scale(canvasWidth / videoWidth, canvasHeight / videoHeight, new canvasScope.Point(0, 0)); // End monitoring code for frames per second\n\n              stats.end();\n              requestAnimationFrame(poseDetectionFrame);\n\n            case 25:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, _callee);\n    }));\n    return _poseDetectionFrame.apply(this, arguments);\n  }\n\n  poseDetectionFrame();\n}\n\nfunction setupCanvas() {\n  mobile = (0, _demoUtils.isMobile)();\n\n  if (mobile) {\n    canvasWidth = Math.min(window.innerWidth, window.innerHeight);\n    canvasHeight = canvasWidth;\n    videoWidth *= 0.7;\n    videoHeight *= 0.7;\n  }\n\n  canvasScope = paper.default;\n  var canvas = document.querySelector('.illustration-canvas');\n  ;\n  canvas.width = canvasWidth;\n  canvas.height = canvasHeight;\n  canvasScope.setup(canvas); // Setup peer illustration canvas\n\n  peerIllustration = new _illustration2.PoseIllustration(canvasScope, {\n    x: 150,\n    y: 150\n  });\n}\n/**\n * Kicks off the demo by loading the posenet model, finding and loading\n * available camera devices, and setting off the detectPoseInRealTime function.\n */\n\n\nfunction bindPage() {\n  return _bindPage.apply(this, arguments);\n}\n\nfunction _bindPage() {\n  _bindPage = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee5() {\n    var t0, info;\n    return regeneratorRuntime.wrap(function _callee5$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            setupCanvas();\n            (0, _demoUtils.toggleLoadingUI)(true);\n            (0, _demoUtils.setStatusText)('Loading PoseNet model...');\n            _context5.next = 5;\n            return posenet_module.load({\n              architecture: defaultPoseNetArchitecture,\n              outputStride: defaultStride,\n              inputResolution: defaultInputResolution,\n              multiplier: defaultMultiplier,\n              quantBytes: defaultQuantBytes\n            });\n\n          case 5:\n            posenet = _context5.sent;\n            (0, _demoUtils.setStatusText)('Loading FaceMesh model...');\n            _context5.next = 9;\n            return facemesh_module.load();\n\n          case 9:\n            facemesh = _context5.sent;\n            (0, _demoUtils.setStatusText)('Loading Avatar file...');\n            t0 = new Date();\n            _context5.next = 14;\n            return parseSVG(Object.values(avatarSvgs).slice(0, 3));\n\n          case 14:\n            (0, _demoUtils.setStatusText)('Setting up camera...');\n            _context5.prev = 15;\n            _context5.next = 18;\n            return loadVideo();\n\n          case 18:\n            video = _context5.sent;\n            _context5.next = 27;\n            break;\n\n          case 21:\n            _context5.prev = 21;\n            _context5.t0 = _context5[\"catch\"](15);\n            info = document.getElementById('info');\n            info.textContent = 'this device type is not supported yet, ' + 'or this browser does not support video capture: ' + _context5.t0.toString();\n            info.style.display = 'block';\n            throw _context5.t0;\n\n          case 27:\n            setupGui([], posenet); // setupFPS();\n\n            (0, _demoUtils.toggleLoadingUI)(false);\n            detectPoseInRealTime(video, posenet);\n\n          case 30:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee5, null, [[15, 21]]);\n  }));\n  return _bindPage.apply(this, arguments);\n}\n\nnavigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;\n\n_fileUtils.FileUtils.setDragDropHandler(function (result) {\n  parseSVG(result);\n});\n\nfunction parseSVG(_x2) {\n  return _parseSVG.apply(this, arguments);\n}\n\nfunction _parseSVG() {\n  _parseSVG = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee6(targets) {\n    var _iterator, _step, target, svgScope, skeleton, _illustration;\n\n    return regeneratorRuntime.wrap(function _callee6$(_context6) {\n      while (1) {\n        switch (_context6.prev = _context6.next) {\n          case 0:\n            _iterator = _createForOfIteratorHelper(targets);\n            _context6.prev = 1;\n\n            _iterator.s();\n\n          case 3:\n            if ((_step = _iterator.n()).done) {\n              _context6.next = 14;\n              break;\n            }\n\n            target = _step.value;\n            _context6.next = 7;\n            return _svgUtils.SVGUtils.importSVG(target\n            /* SVG string or file path */\n            );\n\n          case 7:\n            svgScope = _context6.sent;\n            skeleton = new _skeleton.Skeleton(svgScope);\n            _illustration = new _illustration2.PoseIllustration(canvasScope);\n\n            _illustration.bindSkeleton(skeleton, svgScope);\n\n            illustrations.push(_illustration);\n\n          case 12:\n            _context6.next = 3;\n            break;\n\n          case 14:\n            _context6.next = 19;\n            break;\n\n          case 16:\n            _context6.prev = 16;\n            _context6.t0 = _context6[\"catch\"](1);\n\n            _iterator.e(_context6.t0);\n\n          case 19:\n            _context6.prev = 19;\n\n            _iterator.f();\n\n            return _context6.finish(19);\n\n          case 22:\n            // Notify the peer about the new SVG\n            sendState();\n\n          case 23:\n          case \"end\":\n            return _context6.stop();\n        }\n      }\n    }, _callee6, null, [[1, 16, 19, 22]]);\n  }));\n  return _parseSVG.apply(this, arguments);\n}\n\nbindPage();"},"sourceMaps":null,"error":null,"hash":"563c622677c011e68dc74ebc9548730c","cacheData":{"env":{}}}